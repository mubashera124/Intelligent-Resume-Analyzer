{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2169b828",
   "metadata": {},
   "source": [
    "# Intelligent Resume Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fdb0a",
   "metadata": {},
   "source": [
    "**Context:**\n",
    "\n",
    "Your company receives hundreds of resumes in varying formats, from scanned documents to stylized PDFs. As part of the recruitment automation initiative, your task is to design a prototype system that can extract structured candidate information from unstructured resumes.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Build a Resume Analyzer system that processes resumes and extracts the following structured information for each candidate:\n",
    "\n",
    "Full Name\n",
    "\n",
    "Email Address\n",
    "\n",
    "Phone Number\n",
    "\n",
    "Skills\n",
    "\n",
    "Education\n",
    "\n",
    "School/College/University Name\n",
    "\n",
    "Pass out Year\n",
    "\n",
    "Work Experience\n",
    "\n",
    "Company/Institute Name\n",
    "\n",
    "Starting and Ending Month and Year\n",
    "\n",
    "Total months of experience\n",
    "\n",
    "Projects\n",
    "\n",
    "Certifications (if available)\n",
    "\n",
    "\n",
    "\n",
    "### The system should output the information in JSON format like below:\n",
    "\n",
    "json\n",
    "\n",
    "{\n",
    "  \n",
    "  \"name\": \"Jane Doe\",\n",
    "  \n",
    "  \"email\": \"jane.doe@example.com\",\n",
    "  \n",
    "  \"phone\": \"+1-234-567-8901\",\n",
    "  \n",
    "  \"skills\": [\"Python\", \"Machine Learning\", \"SQL\"],\n",
    "  \n",
    "  \"education\": [...],\n",
    "  \n",
    "  \"experience\": [...],\n",
    "  \n",
    "  \"projects\": [...],\n",
    "  \n",
    "  \"certifications\": [...]\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00ed683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pymupdf  # For reading PDF files\n",
    "import re  # For regular expressions (email, phone extraction)\n",
    "import pytesseract  # For OCR on image files\n",
    "from PIL import Image  # For opening image files\n",
    "import json #To display the output in json format\n",
    "\n",
    "# Function to analyze resume text and extract structured information\n",
    "def resume_analyzer(text):\n",
    "\n",
    "    # Splitting text in to lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # defining Function to extract personal details like name, email, and phone\n",
    "    def extract_personal_info(lines, target_section):\n",
    "        name = lines[0] if lines else ''  # First line is usually the name\n",
    "        emails = []\n",
    "        phones = []\n",
    "\n",
    "        for line in lines:\n",
    "            lower_lines = line.lower()\n",
    "\n",
    "            # Extracting email using regex\n",
    "            emails += re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', lower_lines)\n",
    "\n",
    "            # Extracting phone number using regex\n",
    "            phones += re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', lower_lines)\n",
    "\n",
    "        # Return the requested section\n",
    "        if target_section == 'name':\n",
    "            return name\n",
    "        elif target_section == 'email':\n",
    "            return emails\n",
    "        elif target_section == 'phone':\n",
    "            return phones\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    # Defining keywords for each resume section(Generalizing)\n",
    "    # Because other resumes might have section heading for experience has 'work experience' or education as 'academic background'\n",
    "    SECTION_HEADERS = {\n",
    "        'skills': ['skills', 'technical skills', 'abilities'],\n",
    "        'education': ['education', 'academic background', 'qualifications'],\n",
    "        'experience': ['experience', 'work experience', 'professional experience'],\n",
    "        'projects': ['projects', 'academic projects', 'personal projects'],\n",
    "        'certifications': ['certifications', 'courses', 'licenses'],\n",
    "        'acheivements': ['achievements', 'accomplished']\n",
    "    }\n",
    "\n",
    "    # Defining Function to extract a block of text belonging to a specific section\n",
    "    def extract_section(lines, target_section):\n",
    "        collected_lines = []\n",
    "        collect = False  # Flag to start/stop collecting lines\n",
    "\n",
    "        for l in lines:\n",
    "            lower_line = l.lower()\n",
    "\n",
    "            # Start collecting when section header is found\n",
    "            if any(header in lower_line for header in SECTION_HEADERS[target_section]):\n",
    "                collect = True\n",
    "                continue\n",
    "\n",
    "            # Stop collecting when another section header is found\n",
    "            if collect and any(\n",
    "                any(header in lower_line for header in headers)\n",
    "                for key, headers in SECTION_HEADERS.items() if key != target_section\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # If within the target section, collect the line\n",
    "            if collect:\n",
    "                collected_lines.append(l.strip())\n",
    "\n",
    "        return collected_lines\n",
    "\n",
    "\n",
    "    # Building final structured output in dictionary format\n",
    "    parsed_data = {\n",
    "        \"name\": extract_personal_info(lines, 'name'),\n",
    "        \"email\": extract_personal_info(lines, 'email'),\n",
    "        \"phone\": extract_personal_info(lines, 'phone'),\n",
    "        \"skills\": extract_section(lines, 'skills'),\n",
    "        \"education\": extract_section(lines, 'education'),\n",
    "        \"experience\": extract_section(lines, 'experience'),\n",
    "        \"projects\": extract_section(lines, 'projects'),\n",
    "        \"certifications\": extract_section(lines, 'certifications')\n",
    "    }\n",
    "\n",
    "    # Printing the extracted structured data in readable JSON format\n",
    "\n",
    "    print(json.dumps(parsed_data, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "# Wrapper function to handle different file types (PDF or image)\n",
    "def Intelligent_resume_analyzer(file_path):\n",
    "\n",
    "    file_path_lower = file_path.lower()\n",
    "\n",
    "    # If the file is an image, extract text using OCR\n",
    "    if file_path_lower.endswith(('.jpg', '.png', '.jpeg')):\n",
    "        pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "        img = Image.open(file_path)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        resume_analyzer(text)\n",
    "\n",
    "    # If the file is a PDF, extract text using PyMuPDF\n",
    "    elif file_path_lower.endswith('pdf'):\n",
    "        doc = pymupdf.open(file_path)\n",
    "        for page in doc:\n",
    "            text = page.get_text()  \n",
    "        resume_analyzer(text)\n",
    "\n",
    "    else:\n",
    "        return 'Unknown file'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f45859",
   "metadata": {},
   "source": [
    "### Let's test this on some sample resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d790dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Rajiv Bishnudeo Yadav\",\n",
      "  \"email\": [\n",
      "    \"rajivy1012@gmail.com\",\n",
      "    \"21cs3038@rgipt.ac.in\"\n",
      "  ],\n",
      "  \"phone\": [\n",
      "    \"+91-7385946245\"\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"Programming: Python , C++\",\n",
      "    \"Machine Learning: NLP, Deep Learning, Feature Engineering, Model Development\",\n",
      "    \"Data Science Analytics: NumPy, Pandas, Scikit-learn, PyTorch, Hugging Face Transformers\",\n",
      "    \"Business Intelligence Visualization: Power BI, Matplotlib, Seaborn, Excel\",\n",
      "    \"Databases: SQL (Structured Query Language)\"\n",
      "  ],\n",
      "  \"education\": [\n",
      "    \"•Rajiv Gandhi Institute of Petroleum Technology\",\n",
      "    \"Bachelor of Technology in Computer Science\",\n",
      "    \"CPI: 8.74(Till Sixth Semester)\",\n",
      "    \"•St Paul Science And Commerce Junior College\",\n",
      "    \"07/ 2020\",\n",
      "    \"Higher Secondary Certificate\",\n",
      "    \"Percentage: 84.15\",\n",
      "    \"•The Chanda Devi Saraf School\",\n",
      "    \"06/2018\",\n",
      "    \"Secondary School Certificate\",\n",
      "    \"Percentage: 95.6\"\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    \"• Jaspy Technologies Pvt Limited\",\n",
      "    \"08/2024 - 10/2024\",\n",
      "    \"Data Science Intern\",\n",
      "    \"– Built an AI-powered LinkedIn Profile Optimization Tool leveraging Proxycurl and Gemini API, increasing profile\",\n",
      "    \"scoring accuracy.\",\n",
      "    \"– The tool supports both public profile data collection and PDF uploads, offering detailed scoring and improvement\",\n",
      "    \"suggestions.\",\n",
      "    \"• House Of Couton Private Limited\",\n",
      "    \"02/2024 - 05/2024\",\n",
      "    \"AI/ML Subject Matter Expert Intern\",\n",
      "    \"– Tools & technologies used: Numpy, Pandas, Scikit-learn, Random Forest, Decision Tree, Support Vector Machine\"\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    \"– Applied AI/ML technologies across various domains, including predictive modeling and data analysis, to support\",\n",
      "    \"• Suvidha Foundation\",\n",
      "    \"11/2023 - 12/2023\",\n",
      "    \"Machine Learning Intern\",\n",
      "    \"– Tools & technologies used: Python, Flask, Hugging Face Transformers, Facebook’s BART Large CNN\",\n",
      "    \"– Developed a web application using Facebook’s BART Large CNN model from Hugging Face, enabling users to\",\n",
      "    \"input news articles and generate concise summaries ranging from 30 to 700 words.\",\n",
      "    \"– Designed an NLP pipeline leveraging BART Large CNN for text summarization, improving accuracy of the Model.\",\n",
      "    \"The application uses advanced NLP to provide clear and accessible news summaries.\",\n",
      "    \"• Investment Planning Gen-AI Platform\",\n",
      "    \"01/2025\",\n",
      "    \"– Tools & technologies used: HTML, CSS, MongoDB\",\n",
      "    \"– Developed an AI-driven investment advisory system leveraging Gemini API and machine learning, enabling real-\",\n",
      "    \"time portfolio analysis and personalized financial recommendations.\",\n",
      "    \"• Chess Engine\",\n",
      "    \"04/2024\",\n",
      "    \"– Tools & technologies used: Python,Pygame,Minimax algorithm with alpha-beta pruning,Chess piece evaluation\",\n",
      "    \"functions,Web scraping\",\n",
      "    \"– Engineered an AI-driven chess engine with minimax algorithm and alpha-beta pruning, optimizing move selection,\",\n",
      "    \"alongside an intuitive graphical interface.\"\n",
      "  ],\n",
      "  \"certifications\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#testing on resume in pdf format\n",
    "Intelligent_resume_analyzer('sample_resume2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37197221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"SHRIANSH SINGH\",\n",
      "  \"email\": [\n",
      "    \"shrianshsingh20@gmail.com\"\n",
      "  ],\n",
      "  \"phone\": [\n",
      "    \"+91 9927688832\"\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"industry\",\n",
      "    \"\"\n",
      "  ],\n",
      "  \"education\": [\n",
      "    \"08/2021 - 01/1970 e =6B. Tech in Information Technology\",\n",
      "    \"Pantnagar, Uttarakhand G.B. Pant University of Agriculture and Technology\"\n",
      "  ],\n",
      "  \"experience\": [],\n",
      "  \"projects\": [],\n",
      "  \"certifications\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#testing on image resume\n",
    "Intelligent_resume_analyzer('sample_resume6.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f21059",
   "metadata": {},
   "source": [
    "### 🔍 Reflections & Learnings\n",
    "\n",
    "- Learned to extract text from both PDFs and scanned images using PyMuPDF and Tesseract.\n",
    "- Understood how to use regular expressions to extract emails and phone numbers.\n",
    "- Practiced building modular, reusable code.\n",
    "- Encountered challenges with section detection and solved them by using generalized section headers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
